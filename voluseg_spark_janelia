#!/usr/bin/env python

# set up
import os
import sys
import datetime
from subprocess import run, PIPE

if len(sys.argv) < 3:
    sys.exit('Usage: voluseg_spark_janelia [number-of-nodes] [output-directory]')

n_workers = sys.argv[1]
dir_output = sys.argv[2]

#%%

# define spark commands
spark_class = '/misc/local/spark-versions/spark-3.0.1/bin/spark-class'
deploy_master = 'org.apache.spark.deploy.master.Master'
deploy_workers = 'org.apache.spark.deploy.worker.Worker'
spark_submit = '/usr/local/spark-current/bin/spark-submit'
prepro_script = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'voluseg_submit.py')

# get job name
name_job = int(datetime.datetime.now().timestamp())
name_master = 'master%d'%(name_job)
name_workers = 'worker%d[1-%s]'%(name_job,n_workers)

# launch master
launch_master = run(['bsub','-J',name_master,'-n1',spark_class,deploy_master])
if launch_master.returncode:
    sys.exit('Master was not launched.')

# get all spark job ids
while 1:
    bpeek = run(['bpeek','-J',name_master], stdout=PIPE)
    if 'ALIVE' in bpeek.stdout.decode():
        url_master = 'spark://'+bpeek.stdout.decode().split('spark://')[1].split()[0]
        break

# launch workers
launch_workers = run(['bsub','-J',name_workers,'-n5',spark_class,deploy_workers,url_master])

# run spark
run([spark_submit, prepro_script, n_workers, dir_output, url_master])

# terminate jobs
run(['bkill','-J',name_workers])
run(['bkill','-J',name_master])

