#!/usr/bin/env python

# set up
import os
import sys
import time
import datetime
from subprocess import run, PIPE

if len(sys.argv) < 3:
    sys.exit('Usage: voluseg_spark_janelia [number-of-workers] [output-directory]')

n_workers = sys.argv[1]
dir_output = sys.argv[2]

#%%

# define environment worker variables
name_user = os.getenv('USER')
lsb_jobid = os.getenv('LSB_JOBID')
lsb_jobindex = os.getenv('LSB_JOBINDEX')
os.environ['SPARK_WORKER_DIR'] = '/scratch/%s/spark-%s_%s'%(name_user,lsb_jobid,lsb_jobindex)
os.environ['SPARK_LOCAL_DIRS'] = '/scratch/%s/spark-%s_%s'%(name_user,lsb_jobid,lsb_jobindex)

# define spark commands
spark_class = '/misc/local/spark-versions/spark-3.0.1/bin/spark-class'
deploy_master = 'org.apache.spark.deploy.master.Master'
deploy_workers = 'org.apache.spark.deploy.worker.Worker'
spark_submit = '/misc/local/spark-3.0.1/bin/spark-submit'
prepro_script = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'voluseg_submit.py')

# get job name
timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
name_job = 'voluseg_%s_%s'%(name_user, timestamp)

name_master = 'master_%s'%(name_job)
name_workers = 'worker_%s[1-%s]'%(name_job,n_workers)

# launch master
launch_master = run(['bsub','-W16:00','-J',name_master,'-n1',spark_class,deploy_master])
if launch_master.returncode:
    sys.exit('Master was not launched.')

# get all spark job ids
while 1:
    time.sleep(5)
    bpeek = run(['bpeek','-J',name_master], stdout=PIPE)
    # if master launched
    if 'ALIVE' in bpeek.stdout.decode():
        url_master = 'spark://'+bpeek.stdout.decode().split('spark://')[1].split()[0]
        break
    else:
        print('Waiting for master.')

# launch workers
launch_workers = run(['bsub','-W16:00','-J',name_workers,'-n5',spark_class,deploy_workers,url_master])
if launch_workers.returncode:
    run(['bkill','-J',name_master])
    sys.exit('Workers were not launched.')

while 1:
    time.sleep(5)
    worker_status = run(['bjobs','-J',name_workers], stdout=PIPE)
    # if any workers running
    if all([li.split()[2]=='RUN' for li in worker_status.stdout.decode().split('\n')[1:-1]]):
        break
    else:
        print('Waiting for workers.')

# run spark-submit with prepro_script
run([spark_submit, prepro_script, n_workers, dir_output, url_master])

# terminate jobs
run(['bkill','-J',name_workers])
run(['bkill','-J',name_master])

